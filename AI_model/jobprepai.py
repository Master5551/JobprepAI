# -*- coding: utf-8 -*-
"""JobPrepAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15Q2IS2udy4utuRkfo_2Lc_k007PYOqTn

---


Input Text

---
"""

# Original answer and summarized text
original_answer = "A primary key is a column or a set of columns in a table that uniquely identifies each row or record in that table. It enforces entity integrity and ensures that each record in the table is unique.  A foreign key is a column or a set of columns in a table that establishes a relationship between two tables. It represents a link between two tables based on a relationship between their respective columns. The foreign key in one table is typically a reference to the primary key in another table."

input1 = "A primary key and a foreign key are both key concepts in relational database design, but they serve different purposes.A primary key is a column or a set of columns in a table that uniquely identifies each row or record in that table. A foreign key is a column or a set of columns in a table that establishes a relationship between two tables. It represents a link between two tables based on a relationship between their  columns."

input2 = "In database design, a primary key uniquely identifies each record in a table, ensuring entity integrity. For instance, a student ID column in a student table guarantees each student has a unique identifier. On the other hand, a foreign key establishes relationships between tables by referencing the primary key of another table. For example, in a database with student and course tables, the student ID column in the course table serves as a foreign key referencing the student ID in the student table, indicating enrollment."

input3 = "Primary key gurantees each record in a table must have a unique  value, ensuring data integrity. It simplifies data retrieval process as it provides a quick and efficient way to access specific records within a table And can also be indexed to further enhance query performance, especially in large databases. On the other hand,Foreign keys Ensure that relationships between tables remain valid by enforcing constraints on foreign key values.It enables automatic actions, such as cascading updates or deletes, to maintain data consistency across related tables and also help to optimize query performance by providing information about data relationships"

input4 = """
A primary key is a column or set of columns in a table that uniquely identifies each row or record. It ensures entity integrity by guaranteeing that each record in the table is unique. For instance, in a student table, the student ID column might serve as the primary key because each student should have a distinct ID.

A foreign key, on the other hand, is a column or set of columns in a table that establishes a relationship between two tables. It acts as a link between the tables based on a relationship between their respective columns. Typically, the foreign key in one table references the primary key in another table. For example, in a database with tables for students and courses, the student ID column in the courses table might be a foreign key referencing the student ID column in the students table. This relationship indicates which student is enrolled in which course.
"""
input5 = "Ok Primary key "

# input1 = "Bubble sort is a simple sorting algorithm that works by repeatedly stepping through a list of items to be sorted, comparing each pair of adjacent items, and swapping them if they are in the wrong order. This process continues until no more swaps are needed"

# input2 = "Imagine we have a list of numbers and we want to put them in ascdending or descding order. we  start at the beginning of the list, and if the first number is bigger than the second, we swap them around. Then move to the next pair of numbers and do the same thing.  Keep doing this until we reach the end of the row. This is essentially how bubble sort works. It's a straightforward sorting algorithm that continuously swaps adjacent elements if they're in the wrong order, until the entire list is sorted."

# input3 = "start at one end and compare two values at a time. If the first value should come after the second one, you swap their places. Then, you move one spot over and do the same thing with the next pair of values. Keep going until  every pair is visited. This is one pass through the books.  Repeat this process until a pass can be made without swapping the values. That's when you know the list is organized, or in other words, the list is sorted."

# input4 = "Bubble sort is a simple and efficient sorting algorithm that works by repeatedly selecting the smallest (or largest) element from the unsorted portion of the list and moving it to the sorted portion of the list."

# input5 = "ok"

# original_answer = "Bubble sort is a comparison-based algorithm that sorts items in a list by repeatedly swapping adjacent elements if they are in the wrong order. This process continues until the list is sorted. Despite its simplicity, bubble sort is not typically used for large datasets due to its high computational complexity."

input_text = input4

"""

---


PreProcessing


---

"""

import re
import string
import nltk
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer

def text_preprocessing(text):
    # Removing punctuations
    # remove all characters that are not alphanumeric or whitespace from the text.
    text = re.sub(r'[^\w\s]', '', text)

    # Removing URLs
    text = re.sub(r'http\S+', '', text)

    # Lower casing
    text = text.lower()

    # Tokenization
    tokens = word_tokenize(text)

    # Removing stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]

    # Stemming
    stemmer = PorterStemmer()
    tokens = [stemmer.stem(word) for word in tokens]

    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]

    # Join tokens back into a single string
    preprocessed_text = ' '.join(tokens)

    return preprocessed_text


preprocess_input_text = text_preprocessing(input_text)
print("Original text:")
print(input_text)
print("\nPreprocessed text:")
print(preprocess_input_text)

"""

---

Summarization


---

"""

pip install bert-extractive-summarizer

pip install networkx

pip install transformers

from summarizer import Summarizer
from sumy.summarizers.lsa import LsaSummarizer  #Latent Semantic Analysis
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
import networkx as nx
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
from sumy.summarizers.lex_rank import LexRankSummarizer
from sumy.summarizers.edmundson import EdmundsonSummarizer
from transformers import T5ForConditionalGeneration, T5Tokenizer
nltk.download('punkt')

# Function to summarize text using BERT
def bert_summarize(text, ratio=None, num_sentences=None):
    model = Summarizer()
    if ratio:
        result = model(text, ratio=ratio)
    elif num_sentences:
        result = model(text, num_sentences=num_sentences)
    return result

# Function to summarize text using LSA (sumy)
def lsa_summarize(text, sentences_count=2):
    parser = PlaintextParser.from_string(text, Tokenizer("english"))
    lsa_summarizer = LsaSummarizer()
    summary_sentences = lsa_summarizer(parser.document, sentences_count=sentences_count)
    return ' '.join([str(sentence) for sentence in summary_sentences])

# Function to summarize text using TextRank (NetworkX)
def textrank_summarize(text, sentences_count=2):
    sentences = nltk.sent_tokenize(text)
    graph = nx.Graph()
    for sentence in sentences:
        for other_sentence in sentences:
            if sentence != other_sentence:
                similarity = len(set(sentence.split()) & set(other_sentence.split())) / (len(set(sentence.split())) + len(set(other_sentence.split())))
                graph.add_edge(sentence, other_sentence, weight=similarity)
    ranked_sentences = nx.pagerank(graph)
    summary_sentences = sorted(ranked_sentences, key=ranked_sentences.get, reverse=True)[:sentences_count]
    return ' '.join([str(sentence) for sentence in summary_sentences])

# Function to summarize text using TF-IDF
def tfidf_summarize(text, num_terms=2):
    tfidf_vectorizer = TfidfVectorizer()
    tfidf_matrix = tfidf_vectorizer.fit_transform([text])
    feature_names = tfidf_vectorizer.get_feature_names_out()
    top_indices = (-tfidf_matrix.toarray()[0]).argsort()[:num_terms]
    summary_terms = [feature_names[index] for index in top_indices]
    return summary_terms

# Function to summarize text using LexRank (sumy)
def lexrank_summarize(text, sentences_count=2):
    parser = PlaintextParser.from_string(text, Tokenizer("english"))
    lexrank_summarizer = LexRankSummarizer()
    summary_sentences = lexrank_summarizer(parser.document, sentences_count=sentences_count)
    return ' '.join([str(sentence) for sentence in summary_sentences])

# Function to summarize text using Edmundson Summarization (sumy)
def edmundson_summarize(text, sentences_count=2):
    summarizer = EdmundsonSummarizer()
    summarizer.bonus_words = ('data', 'structures')  # Specify bonus words
    summarizer.stigma_words = ('not',)  # Specify stigma words
    summarizer.null_words = ('just', 'example')  # Specify null words
    parser = PlaintextParser.from_string(text, Tokenizer("english"))
    summary_sentences = summarizer(parser.document, sentences_count=sentences_count)
    return ' '.join([str(sentence) for sentence in summary_sentences])

# Function to summarize text using T5-Small
def t5_summarize(input_text):
    model_name = "t5-small"
    model = T5ForConditionalGeneration.from_pretrained(model_name)
    tokenizer = T5Tokenizer.from_pretrained(model_name)
    inputs = tokenizer.encode("summarize: " + input_text, return_tensors="pt", max_length=512, truncation=True)
    summary_ids = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary


# Summarize the text using different methods
bert_summary = bert_summarize(input_text, ratio=0.1)
lsa_summary = lsa_summarize(input_text)
textrank_summary = textrank_summarize(input_text)
tfidf_summary = tfidf_summarize(input_text)
lexrank_summary = lexrank_summarize(input_text)
edmundson_summary = edmundson_summarize(input_text)
t5_summary = t5_summarize(input_text)  # Summarize using T5-Small

# Calculate the length of the original text
original_text_length = len(input_text)

# Define the percentage of the original text size to display
percentage_to_show = 10  # Show top 10% summaries

# Calculate the number of summaries to show
num_summaries_to_show = int((percentage_to_show / 100) * original_text_length)

print("Orignal Text : ")
print(input_text)
# Display the summaries
print("BERT Summary:")
print(bert_summary)
print("\nLSA Summarizer Summary:")
print(lsa_summary)
print("\nTextRank Summary:")
print(textrank_summary)
print("\nTF-IDF Summary:")
print(tfidf_summary)
print("\nLexRank Summary:")
print(lexrank_summary)
print("\nEdmundson Summarization Summary:")
print(edmundson_summary)
print("\nT5-Small Summary:")
print(t5_summary)

from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import CountVectorizer
from nltk.metrics import jaccard_distance

# Calculate similarity using cosine similarity
def calculate_cosine_similarity(text1, text2):
    vectorizer = CountVectorizer().fit_transform([text1, text2])
    vectors = vectorizer.toarray()
    return cosine_similarity([vectors[0]], [vectors[1]])[0][0]

# Calculate Jaccard similarity
def calculate_jaccard_similarity(text1, text2):
    set1 = set(text1.split())
    set2 = set(text2.split())
    return 1 - jaccard_distance(set1, set2)

# Calculate similarity between original answer and summarized text for all summaries
summaries = [bert_summary, lsa_summary, textrank_summary, tfidf_summary, lexrank_summary, edmundson_summary, t5_summary]
similarity_scores = []

for summary in summaries:
    # If the summary is a list of sentences, join them into a single string
    if isinstance(summary, list):
        summary = ' '.join(summary)
    jaccard_similarity = calculate_jaccard_similarity(original_answer, summary)
    cosine_similarity_score = calculate_cosine_similarity(original_answer, summary)
    similarity_scores.append((jaccard_similarity, cosine_similarity_score))

# Print similarity scores
methods = ["BERT", "LSA", "TextRank", "TF-IDF", "LexRank", "Edmundson", "T5-Small"]

print("Similarity Scores:")
for method, (jaccard_sim, cosine_sim) in zip(methods, similarity_scores):
    print(f"{method}:\nJaccard Similarity: {jaccard_sim}\nCosine Similarity: {cosine_sim}\n")

"""

---


key phrase extraction


---

"""

pip install networkx nltk spacy

pip install git+https://github.com/boudinfl/pke.git

pip install rake-nltk

!pip install sentence-transformers

pip install yake

import string
import nltk
from nltk.corpus import stopwords as nltk_stopwords
import pke
from rake_nltk import Rake
import yake
import networkx as nx
from transformers import pipeline
from collections import defaultdict

# Download NLTK resources
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('punkt')

# Define the set of stop words
stopwords = list(string.punctuation)
stopwords += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']
stopwords += nltk_stopwords.words('english')

# Remove stop words and apply lemmatization
def preprocess_text(text):
    # Tokenization
    tokens = nltk.word_tokenize(text)
    # Remove stop words and punctuation
    tokens = [word for word in tokens if word.lower() not in stopwords and word.isalpha()]
    # Lemmatization
    lemmatizer = nltk.WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return " ".join(tokens)

# Extract top keywords using TextRank
def textrank(text):
    extractor = pke.unsupervised.TextRank()
    extractor.load_document(input=text, language='en')
    extractor.candidate_selection(pos={'NOUN', 'PROPN', 'ADJ'})
    extractor.candidate_weighting(window=10, pos={'NOUN', 'PROPN', 'ADJ'})
    keyphrases = extractor.get_n_best(n=10)
    return [keyphrase[0] for keyphrase in keyphrases]

# Extract top keywords using RAKE
def rake(text):
    r = Rake()
    r.extract_keywords_from_text(text)
    keyphrases = r.get_ranked_phrases_with_scores()
    return [keyphrase[1] for keyphrase in keyphrases[:10]]

# Extract top keywords using YAKE
def yake_extraction(text):
    custom_kw_extractor = yake.KeywordExtractor(lan="en", n=3, dedupLim=0.9, top=10, features=None)
    keywords = custom_kw_extractor.extract_keywords(text)
    return [kw[0] for kw in keywords]

# Implement PatternRank
def patternrank(text):
    def preprocess_text(text):
        return text.lower().split()

    def build_graph(words, window_size=2):
        graph = defaultdict(int)
        for i in range(len(words)):
            for j in range(i + 1, min(i + window_size + 1, len(words))):
                if words[i] != words[j]:
                    graph[(words[i], words[j])] += 1
                    graph[(words[j], words[i])] += 1
        return graph

    words = preprocess_text(text)
    graph = build_graph(words, window_size=2)

    G = nx.Graph()
    for edge, weight in graph.items():
        G.add_edge(edge[0], edge[1], weight=weight)

    pagerank = nx.pagerank(G)

    sorted_nodes = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)

    return [keyword[0] for keyword in sorted_nodes[:10]]

# Implement PromptRank
def prompt_rank(text, prompt_template, max_keywords=10):
    # Initialize the keyword extraction pipeline with the specified model
    keyword_extractor = pipeline("text-generation", model="bert-base-uncased")

    # Generate prompts for each word in the text
    prompts = [prompt_template.format(word=word) for word in text.split()]

    # Score each prompt and word pair
    keyword_scores = {}
    for word, prompt in zip(text.split(), prompts):
        result = keyword_extractor(prompt, max_length=50, num_return_sequences=1, num_beams=5)
        keyword_scores[word] = result[0]['generated_text']

    # Sort words by their relevance score
    sorted_keywords = sorted(keyword_scores.items(), key=lambda x: x[1], reverse=True)[:max_keywords]

    return [keyword[0] for keyword in sorted_keywords]

# Extract top keywords using TopicRank
def topicrank(text):
    extractor = pke.unsupervised.TopicRank()
    extractor.load_document(input=text, language='en')
    extractor.candidate_selection()
    extractor.candidate_weighting()
    keyphrases = extractor.get_n_best(n=10)
    return [keyphrase[0] for keyphrase in keyphrases]

# Extract top keywords using SingleRank
def singlerank(text):
    extractor = pke.unsupervised.SingleRank()
    extractor.load_document(input=text, language='en')
    extractor.candidate_selection(pos={'NOUN'})
    extractor.candidate_weighting()
    keyphrases = extractor.get_n_best(n=10)
    return [keyphrase[0] for keyphrase in keyphrases]

# Extract top keywords using PositionRank
def positionrank(text):
    extractor = pke.unsupervised.PositionRank()
    extractor.load_document(input=text, language='en')
    extractor.candidate_selection()
    extractor.candidate_weighting(window=10)
    keyphrases = extractor.get_n_best(n=10)
    return [keyphrase[0] for keyphrase in keyphrases]

# Extract top keywords using KPMiner
def kpminer(text):
    extractor = pke.unsupervised.KPMiner()
    extractor.load_document(input=text, language='en')
    extractor.candidate_selection()
    extractor.candidate_weighting()
    keyphrases = extractor.get_n_best(n=10)
    return [keyphrase[0] for keyphrase in keyphrases]

# Extract top keywords using MultipartiteRank
def multipartiterank(text):
    extractor = pke.unsupervised.MultipartiteRank()
    extractor.load_document(input=text, language='en')
    extractor.candidate_selection()
    extractor.candidate_weighting(alpha=1.1, threshold=0.74, method='average')
    keyphrases = extractor.get_n_best(n=10)
    return [keyphrase[0] for keyphrase in keyphrases]

# Example text
text = "A primary key is a column or a set of columns in a table that uniquely identifies each row or record in that table. It enforces entity integrity and ensures that each record in the table is unique. And a foreign key is a column or a set of columns in a table that establishes a relationship between two tables. It represents a link between two tables based on a relationship between their respective columns. The foreign key in one table is typically a reference to the primary key in another table. This establishes a relationship between the two tables, indicating which student is enrolled in which course."

# Preprocess text
preprocessed_text = preprocess_text(text)

# Extract keywords using all methods
text_rank_keywords = textrank(preprocessed_text)
print("TextRank Keywords:", text_rank_keywords)
rake_keywords = rake(text)
print("RAKE Keywords:", rake_keywords)
yake_keywords = yake_extraction(text)
print("YAKE Keywords:", yake_keywords)
patternrank_keywords = patternrank(preprocessed_text)
print("PatternRank Keywords:", patternrank_keywords)
topicrank_keywords = topicrank(preprocessed_text)
print("TopicRank Keywords:", topicrank_keywords)
singlerank_keywords = singlerank(preprocessed_text)
print("SingleRank Keywords:", singlerank_keywords)
positionrank_keywords = positionrank(preprocessed_text)
print("PositionRank Keywords:", positionrank_keywords)
kpminer_keywords = kpminer(preprocessed_text)
print("KPMiner Keywords:", kpminer_keywords)
multipartiterank_keywords = multipartiterank(preprocessed_text)
print("MultipartiteRank Keywords:", multipartiterank_keywords)
prompt_template = "Extract keywords related to '{word}':"
#prompt_keywords = prompt_rank(preprocessed_text, prompt_template)

import string
import pke
import nltk
from nltk.corpus import stopwords as nltk_stopwords
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import tensorflow_hub as hub
import numpy as np
from summarizer import Summarizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd

# Define the set of stop words
nltk.download('stopwords')
stopwords = list(string.punctuation)
stopwords += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']
stopwords += nltk_stopwords.words('english')
model = hub.load("https://tfhub.dev/google/universal-sentence-encoder/4")

def semantic_similarity(text1, text2):
    # Generate embeddings for the texts
    embedding1 = model([text1])[0]
    embedding2 = model([text2])[0]

    # Calculate and return the cosine similarity of the embeddings
    return np.inner(embedding1, embedding2)

# Function to extract keyphrases using TextRank
def extract_keyphrases(text, max_length=2):
    extractor = pke.unsupervised.TextRank()
    extractor.load_document(input=text, language='en')
    extractor.candidate_selection(pos={'NOUN', 'PROPN', 'ADJ'})
    extractor.candidate_weighting(window=10, pos={'NOUN', 'PROPN', 'ADJ'})

    # Get keyphrases and filter based on length
    keyphrases = extractor.get_n_best(n=10)
    keyphrases = [kp for kp in keyphrases if len(kp[0].split()) <= max_length]

    return keyphrases

# Function to calculate Jaccard similarity
def jaccard_similarity(list1, list2):
    s1 = set(list1)
    s2 = set(list2)
    return len(s1.intersection(s2)) / len(s1.union(s2))

# Function to summarize text using BERT
def bert_summarize(text, ratio=None, num_sentences=None):
    model = Summarizer()
    if ratio:
        result = model(text, ratio=ratio)
    elif num_sentences:
        result = model(text, num_sentences=num_sentences)
    return result

# Function to calculate cosine similarity
def calculate_cosine_similarity(text1, text2):
    vectorizer = CountVectorizer().fit_transform([text1, text2])
    vectors = vectorizer.toarray()
    return cosine_similarity([vectors[0]], [vectors[1]])[0][0]

keyphrases1 = [kp[0] for kp in extract_keyphrases(original_answer)]

# Summarize the text using BERT
bert_summary_original = bert_summarize(original_answer, ratio=0.1)

# Calculate cosine similarity between original answer and BERT summary
cosine_similarity_original = calculate_cosine_similarity(original_answer, bert_summary_original)

# Create DataFrame for the results
data = {
    "Document": ["Original Answer", "Input 1", "Input 2", "Input 3", "Input 4","Input 5"],
    "Cosine Similarity": [cosine_similarity_original],
    "Jaccard Similarity": [None],
    "Semantic Similarity": [None]
}

# Calculate Jaccard similarity and Semantic similarity for input 1, 2, 3, and 4
for input_text in [input1, input2, input3, input4,input5]:
    keyphrases_input = [kp[0] for kp in extract_keyphrases(input_text)]
    similarity_jaccard = jaccard_similarity(keyphrases1, keyphrases_input)
    similarity_semantic = semantic_similarity(original_answer, input_text)
    data["Jaccard Similarity"].append(similarity_jaccard)
    data["Semantic Similarity"].append(similarity_semantic)
    bert_summary_input = bert_summarize(input_text, ratio=0.1)
    cosine_similarity_input = calculate_cosine_similarity(original_answer, bert_summary_input)
    data["Cosine Similarity"].append(cosine_similarity_input)

df = pd.DataFrame(data)
# Assign weights to each similarity metric
weight_cosine = 0.2
weight_jaccard = 0.3
weight_semantic = 0.5

# Calculate final score out of 200
for index, row in df.iterrows():
    cosine_similarity_score = row["Cosine Similarity"]
    jaccard_similarity_score = row["Jaccard Similarity"]
    semantic_similarity_score = row["Semantic Similarity"]
    final_score = (
        (cosine_similarity_score * weight_cosine) +
        (jaccard_similarity_score * weight_jaccard) +
        (semantic_similarity_score * weight_semantic)
    ) * 100
    df.at[index, "Final Score"] = final_score

# Display the DataFrame
print(df)

import string
import nltk
from nltk.corpus import stopwords as nltk_stopwords
import pke
from rake_nltk import Rake
import yake
import networkx as nx
from transformers import pipeline
from collections import defaultdict
from sklearn.feature_extraction.text import TfidfVectorizer
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer
from sumy.summarizers.lex_rank import LexRankSummarizer
from sumy.summarizers.edmundson import EdmundsonSummarizer
from summarizer import Summarizer
from transformers import T5Tokenizer, T5ForConditionalGeneration
import pandas as pd
from nltk.corpus import wordnet as wn
from nltk.metrics import jaccard_distance
from sklearn.metrics.pairwise import cosine_similarity

# Download NLTK resources
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('punkt')

# Define the set of stop words
stopwords = list(string.punctuation)
stopwords += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']
stopwords += nltk_stopwords.words('english')

# Preprocess text: remove stop words, punctuation, and apply lemmatization
def preprocess_text(text):
    # Tokenization
    tokens = nltk.word_tokenize(text)
    # Remove stop words and punctuation
    tokens = [word for word in tokens if word.lower() not in stopwords and word.isalpha()]
    # Lemmatization
    lemmatizer = nltk.WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return " ".join(tokens)

# Summarization Methods

def bert_summarize(text, ratio=None, num_sentences=None):
    model = Summarizer()
    if ratio:
        result = model(text, ratio=ratio)
    elif num_sentences:
        result = model(text, num_sentences=num_sentences)
    else:
        result = model(text)  # Add this line to handle the case when neither ratio nor num_sentences is specified
    return result

# Function to summarize text using LSA (sumy)
def lsa_summarize(text, sentences_count=2):
    parser = PlaintextParser.from_string(text, Tokenizer("english"))
    lsa_summarizer = LsaSummarizer()
    summary_sentences = lsa_summarizer(parser.document, sentences_count=sentences_count)
    return ' '.join([str(sentence) for sentence in summary_sentences])

# Function to summarize text using TextRank (NetworkX)
def textrank_summarize(text, sentences_count=2):
    sentences = nltk.sent_tokenize(text)
    graph = nx.Graph()
    for sentence in sentences:
        for other_sentence in sentences:
            if sentence != other_sentence:
                similarity = len(set(sentence.split()) & set(other_sentence.split())) / (len(set(sentence.split())) + len(set(other_sentence.split())))
                graph.add_edge(sentence, other_sentence, weight=similarity)
    ranked_sentences = nx.pagerank(graph)
    summary_sentences = sorted(ranked_sentences, key=ranked_sentences.get, reverse=True)[:sentences_count]
    return ' '.join([str(sentence) for sentence in summary_sentences])

# Function to summarize text using TF-IDF
def tfidf_summarize(text, num_terms=2):
    tfidf_vectorizer = TfidfVectorizer()
    tfidf_matrix = tfidf_vectorizer.fit_transform([text])
    feature_names = tfidf_vectorizer.get_feature_names_out()
    top_indices = (-tfidf_matrix.toarray()[0]).argsort()[:num_terms]
    summary_terms = [feature_names[index] for index in top_indices]
    return summary_terms

# Function to summarize text using LexRank (sumy)
def lexrank_summarize(text, sentences_count=2):
    parser = PlaintextParser.from_string(text, Tokenizer("english"))
    lexrank_summarizer = LexRankSummarizer()
    summary_sentences = lexrank_summarizer(parser.document, sentences_count=sentences_count)
    return ' '.join([str(sentence) for sentence in summary_sentences])

# Function to summarize text using Edmundson Summarization (sumy)
def edmundson_summarize(text, sentences_count=2):
    summarizer = EdmundsonSummarizer()
    summarizer.bonus_words = ('data', 'structures')  # Specify bonus words
    summarizer.stigma_words = ('not',)  # Specify stigma words
    summarizer.null_words = ('just', 'example')  # Specify null words
    parser = PlaintextParser.from_string(text, Tokenizer("english"))
    summary_sentences = summarizer(parser.document, sentences_count=sentences_count)
    return ' '.join([str(sentence) for sentence in summary_sentences])

# Function to summarize text using T5-Small
def t5_summarize(input_text):
    model_name = "t5-small"
    model = T5ForConditionalGeneration.from_pretrained(model_name)
    tokenizer = T5Tokenizer.from_pretrained(model_name)
    inputs = tokenizer.encode("summarize: " + input_text, return_tensors="pt", max_length=512, truncation=True)
    summary_ids = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

# Keyword Extraction Methods

# Extract top keywords using TextRank
def textrank_keywords(text):
    extractor = pke.unsupervised.TextRank()
    extractor.load_document(input=text, language='en')
    extractor.candidate_selection()
    extractor.candidate_weighting()
    keyphrases = extractor.get_n_best(n=10)
    return [keyphrase[0] for keyphrase in keyphrases]

# Extract top keywords using RAKE
def rake_keywords(text):
    r = Rake()
    r.extract_keywords_from_text(text)
    keyphrases = r.get_ranked_phrases_with_scores()
    return [keyphrase[1] for keyphrase in keyphrases[:10]]

# Extract top keywords using YAKE
def yake_keywords(text):
    custom_kw_extractor = yake.KeywordExtractor(lan="en", n=3, dedupLim=0.9, top=10, features=None)
    keywords = custom_kw_extractor.extract_keywords(text)
    return [kw[0] for kw in keywords]

# Extract top keywords using PatternRank
def patternrank_keywords(text):
    # Define the preprocess_text function
    def preprocess_text(text):
        return text.lower().split()

    def build_graph(words, window_size=2):
        graph = defaultdict(int)
        for i in range(len(words)):
            for j in range(i + 1, min(i + window_size + 1, len(words))):
                if words[i] != words[j]:
                    graph[(words[i], words[j])] += 1
                    graph[(words[j], words[i])] += 1
        return graph

    words = preprocess_text(text)
    graph = build_graph(words, window_size=2)

    G = nx.Graph()
    for edge, weight in graph.items():
        G.add_edge(edge[0], edge[1], weight=weight)

    pagerank = nx.pagerank(G)

    sorted_nodes = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)

    return [keyword[0] for keyword in sorted_nodes[:10]]

# Extract top keywords using TopicRank
def topicrank_keywords(text):
    extractor = pke.unsupervised.TopicRank()
    extractor.load_document(input=text, language='en')
    extractor.candidate_selection()
    extractor.candidate_weighting()
    keyphrases = extractor.get_n_best(n=10)
    return [keyphrase[0] for keyphrase in keyphrases]

# Extract top keywords using SingleRank
def singlerank_keywords(text):
    extractor = pke.unsupervised.SingleRank()
    extractor.load_document(input=text, language='en')
    extractor.candidate_selection()
    extractor.candidate_weighting()
    keyphrases = extractor.get_n_best(n=10)
    return [keyphrase[0] for keyphrase in keyphrases]

# Extract top keywords using PositionRank
def positionrank_keywords(text):
    extractor = pke.unsupervised.PositionRank()
    extractor.load_document(input=text, language='en')
    extractor.candidate_selection()
    extractor.candidate_weighting()
    keyphrases = extractor.get_n_best(n=10)
    return [keyphrase[0] for keyphrase in keyphrases]

# Extract top keywords using KPMiner
def kpminer_keywords(text):
    extractor = pke.unsupervised.KPMiner()
    extractor.load_document(input=text, language='en')
    extractor.candidate_selection()
    extractor.candidate_weighting()
    keyphrases = extractor.get_n_best(n=10)
    return [keyphrase[0] for keyphrase in keyphrases]

# Extract top keywords using MultipartiteRank
def multipartiterank_keywords(text):
    extractor = pke.unsupervised.MultipartiteRank()
    extractor.load_document(input=text, language='en')
    extractor.candidate_selection()
    extractor.candidate_weighting(alpha=1.1, threshold=0.74, method='average')
    keyphrases = extractor.get_n_best(n=10)
    return [keyphrase[0] for keyphrase in keyphrases]

# Extract top keywords using PromptRank
def prompt_rank_keywords(text, prompt_template="Extract keywords related to '{word}':"):
    keyword_extractor = pipeline("text-generation", model="bert-base-uncased")

    # Generate prompts for each word in the text
    prompts = [prompt_template.format(word=word) for word in text.split()]

    # Score each prompt and word pair
    keyword_scores = {}
    for word, prompt in zip(text.split(), prompts):
        result = keyword_extractor(prompt, max_length=50, num_return_sequences=1, num_beams=5)
        keyword_scores[word] = result[0]['generated_text']

    # Sort words by their relevance score
    sorted_keywords = sorted(keyword_scores.items(), key=lambda x: x[1], reverse=True)[:10]

    return [keyword[0] for keyword in sorted_keywords]

def calculate_cosine_similarity(text1, text2):
    if isinstance(text1, list):
        text1 = ' '.join(text1)
    if isinstance(text2, list):
        text2 = ' '.join(text2)
    vectorizer = TfidfVectorizer().fit_transform([text1, text2])
    vectors = vectorizer.toarray()
    return cosine_similarity([vectors[0]], [vectors[1]])[0][0]


# Define the original answer and user's answer

original_answer = "A primary key is a column or a set of columns in a table that uniquely identifies each row or record in that table. It enforces entity integrity and ensures that each record in the table is unique.  A foreign key is a column or a set of columns in a table that establishes a relationship between two tables. It represents a link between two tables based on a relationship between their respective columns. The foreign key in one table is typically a reference to the primary key in another table."
user_answer = """
A primary key is a column or set of columns in a table that uniquely identifies each row or record. It ensures entity integrity by guaranteeing that each record in the table is unique. For instance, in a student table, the student ID column might serve as the primary key because each student should have a distinct ID.

A foreign key, on the other hand, is a column or set of columns in a table that establishes a relationship between two tables. It acts as a link between the tables based on a relationship between their respective columns. Typically, the foreign key in one table references the primary key in another table. For example, in a database with tables for students and courses, the student ID column in the courses table might be a foreign key referencing the student ID column in the students table. This relationship indicates which student is enrolled in which course.
"""

# Preprocess both answers
original_answer_processed = preprocess_text(original_answer)
user_answer_processed = preprocess_text(user_answer)

# List of summarization methods
summarization_methods = [
    ("BERT", bert_summarize),
    ("LSA", lsa_summarize),
    ("TextRank", textrank_summarize),
    ("TF-IDF", tfidf_summarize),
    ("LexRank", lexrank_summarize),
    ("Edmundson", edmundson_summarize),
    ("T5-Small", t5_summarize)
]

# List of keyword extraction methods
keyword_extraction_methods = [
    ("TextRank", textrank_keywords),
    ("RAKE", rake_keywords),
    ("YAKE", yake_keywords),
    ("PatternRank", patternrank_keywords),
    ("TopicRank", topicrank_keywords),
    ("SingleRank", singlerank_keywords),
    ("PositionRank", positionrank_keywords),
    ("KPMiner", kpminer_keywords),
    ("MultipartiteRank", multipartiterank_keywords),
    # ("PromptRank", prompt_rank_keywords)
]

# Dictionary to store results
results = []

# Iterate over each summarization method
for summarization_method, summarizer_func in summarization_methods:

    # Summarize user's answer
    user_summary = summarizer_func(user_answer)

    # Calculate cosine similarity between original and user's summaries
    cosine_sim = calculate_cosine_similarity(original_answer,user_summary)

    # Iterate over each keyword extraction method
    for keyword_method, keyword_extractor_func in keyword_extraction_methods:
        # Extract keywords from original and user's answers
        original_keywords = keyword_extractor_func(original_answer_processed)
        user_keywords = keyword_extractor_func(user_answer_processed)

        # Calculate Jaccard similarity between original and user's keywords
        if original_keywords and user_keywords:  # Check if both lists are not empty
            jaccard_sim = 1 - jaccard_distance(set(original_keywords), set(user_keywords))
        else:
            jaccard_sim = 0.1  # Assign a small value if one of the lists is empty

        # Calculate semantic similarity between original and user's keywords
        if original_keywords and user_keywords:  # Check if both lists are not empty
            semantic_sim = max(
                wn.synsets(original_keyword)[0].wup_similarity(wn.synsets(user_keyword)[0])
                if wn.synsets(original_keyword) and wn.synsets(user_keyword)
                else 0.1  # Assign a small value if no synsets exist
                for original_keyword in original_keywords
                for user_keyword in user_keywords
            )
        else:
            semantic_sim = 0.1  # Assign a small value if one of the lists is empty

        # Calculate final score
        final_score = (0.2 * cosine_sim) + (0.3 * jaccard_sim) + (0.5 * semantic_sim)

        # Append results to the list
        results.append({
            "Summarization Method": summarization_method,
            "Keyword Extraction Method": keyword_method,
            "Cosine Similarity": cosine_sim,
            "Jaccard Similarity": jaccard_sim,
            "Semantic Similarity": semantic_sim,
            "Final Score": final_score
        })
        print(results)


# Convert results to DataFrame
results_df = pd.DataFrame(results)

# Save results to Excel
results_df.to_excel("summarization_keyword_extraction_results.xlsx", index=False)

pip install scikit-learn

pip install xlsxwriter

import pandas as pd

# Convert results to DataFrame
results_df = pd.DataFrame(results)

# Save the DataFrame to a CSV file
results_df.to_csv("summarization_keyword_extraction_results.csv", index=False)

